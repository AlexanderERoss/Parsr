import argparse
import os

import pandas as pd
from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN
from collections import Counter
from sklearn import metrics
from sklearn.feature_selection import RFECV
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn_porter import Porter

parser = argparse.ArgumentParser(description='Train a decision tree to recognize headings.')
parser.add_argument('dataset_dir', help='folder containing the .csv files generated by build_dataset.py')
parser.add_argument('out_dir', help='folder in which to save the trained model')
args = parser.parse_args()

dataset_dir = args.dataset_dir
paths = os.listdir(dataset_dir)
X = []
y = []

for path in paths:
    df = pd.read_csv(os.path.join(dataset_dir, path), header=0)

    if len(df) < 3:
        continue
    
    df['is_different_style'] = df['is_different_style'].apply(lambda x: 1 if x else 0)
    df['is_font_bigger'] = df['is_font_bigger'].apply(lambda x: 1 if x else 0)
    df['is_font_unique'] = df['is_font_unique'].apply(lambda x: 1 if x else 0)
    df['different_color'] = df['different_color'].apply(lambda x: 1 if x else 0)
    df['label'] = df['label'].apply(lambda x: 1 if x == 'heading' else 0)

    for i in range(len(df)):
        X.append([df['is_different_style'][i], df['is_font_bigger'][i],
                  df['is_font_unique'][i]
                 ])
            
    y = y + list(df['label'])


# Splitting dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

print('Original training set shape %s' % Counter(y_train))
X_train_res, y_train_res = SMOTE(random_state=42).fit_resample(X_train, y_train)
print('Resampled training set shape %s' % Counter(y_train_res))

# fitting
parameters = {'min_samples_leaf':[1,2,3], 'min_samples_split':[2,3], 'criterion':['entropy','gini']}
clf_cv = GridSearchCV(DecisionTreeClassifier(), parameters).fit(X_train_res, y_train_res)
clf = DecisionTreeClassifier(min_samples_leaf=clf_cv.best_params_['min_samples_leaf'], min_samples_split=clf_cv.best_params_['min_samples_split'], criterion=clf_cv.best_params_['criterion'], splitter='best')
selector = RFECV(clf, step=1, cv=10, scoring=metrics.make_scorer(metrics.f1_score))
selector = selector.fit(X_train_res, y_train_res)

# overfitting
X_res, y_res = SMOTE(random_state=42).fit_resample(X, y)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X_res, y_res, test_size=0.2)
parameters = {'min_samples_leaf':[1,2,3], 'min_samples_split':[2,3], 'criterion':['entropy','gini']}
clf_cv = GridSearchCV(DecisionTreeClassifier(), parameters).fit(X_res, y_res)
clf = DecisionTreeClassifier(min_samples_leaf=clf_cv.best_params_['min_samples_leaf'], min_samples_split=clf_cv.best_params_['min_samples_split'], criterion=clf_cv.best_params_['criterion'], splitter='best')
selector = RFECV(clf, step=1, cv=10, scoring=metrics.make_scorer(metrics.f1_score))
selector = selector.fit(X_res, y_res)

y_pred = selector.predict(X_test2)

print('precision:', metrics.precision_score(y_test2, y_pred))
print('recall:', metrics.recall_score(y_test2, y_pred))
print('f1:', metrics.f1_score(y_test2, y_pred))

porter = Porter(selector.estimator_, language='js')
output = porter.export(embed_data=True)

with open(os.path.join(args.out_dir, 'model.js'), mode='w+', encoding='utf8') as f:
    f.write('export ' + output)
